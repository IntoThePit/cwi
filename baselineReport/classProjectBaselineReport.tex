%
% File coling2018.tex
%
% Contact: zhu2048@gmail.com & liuzy@tsinghua.edu.cn
%% Based on the style files for COLING-2016, which were, in turn,
%% Based on the style files for COLING-2014, which were, in turn,
%% Based on the style files for ACL-2014, which were, in turn,
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{coling2018}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}

\setlength\titlebox{1cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.


\title{Baseline Progress Report}

\begin{document}
\maketitle

\section{Progress so far}

For my baseline system, I focused on features that can be extracted using only the target word; no context words were considered. First, I improved the basic architecture of the provided baseline to allow easy pattern-matching of prefixes, infixes and suffixes, and for further features of this type to be easily added.


My initial hypotheses to try for the baseline were as follows:
\begin{enumerate}
\item Letter rarity - because words with rarer letters might be harder to understand.
\item Max consecutive consonants, max consecutive vowels - because dense combinations of letters might be difficult to parse
\item Uniquely English vowel/consonant diagraphs/consonant blends - because a non-English speaker might find these tricky.
\item POS tags - because some POS are part of closed sets and so potentially easier to understand
\item Number of synonyms - because ambiguous words might be more confusing
\end{enumerate}

All of these features were stacked together and run through the basic linear regression.

While individually, these features did allow the system to predict with greater-than-random accuracy, most of them did not improve over the initial baseline. Some of the features, such as consecutive vowels, reduced the testing accuracy if they were added. This may just be noise in the data, or it may be that learning is made more difficult if irrelevant features are added.

I also experimented with word embeddings, but have not yet implemented them to a working level.

\section{What next}

With all of my features and the Random Forest Classifier, my system had a macro-F1 score of 0.81 and 0.74 for English and Spanish respectively, which are both improvements over the basic baseline of 0.69 and 0.72.

One of the more obvious additions would be to add context to my currently context-free features. Given the fact that humans frequently infer meaning from context, it seems that this could be a fruitful way to increase performance. There are various ways to achieve this increased context, such as looking at N-grams, semantic trees, or using a Machine Learning approach that has 'memory'.


Many of my features are approximations for the rarity of a word, so it might be worth skimming Wikipedia or other corpuses for this information.


I briefly tried a few different Machine Learning techniques, such as scikit's Random Forest Classifier, which provided significant improvements to the overall scores. Given the fact that this approach is not even suited to the large number of features I was feeding it, this shows some promise for making improvements that are tuned to the specific Machine Learning Approach. 

Thus, my next step would be to explore the different Machine Learning approaches in a more rigorous way, thinking more precisely about which features to use and how to process them.

\end{document}
